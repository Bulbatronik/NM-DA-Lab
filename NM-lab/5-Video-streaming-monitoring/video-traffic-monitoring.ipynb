{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drag and drop the following files in your colab:\n",
        "\n",
        "- Live_capture_NetLabMeas.csv\n",
        "- dns_pcap_yt_s_1_1005.pcap.log\n",
        "- min_out_pcap_yt_s_1_1005.pcap.log\n",
        "- requests_yt_s_1_1005.log\n",
        "- yt_s_1_1005.log\n",
        "\n",
        "Import the 'Live_Capture_NetLabMeas.csv' file into a dataframe. Inspect its content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/Network measurements /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqMxmTliXrde"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7v8v3WrX1Q0"
      },
      "source": [
        "# **Monitoring Youtube Video Streaming Traffic**\n",
        "\n",
        "Today we will complete the following task:\n",
        "\n",
        "\n",
        "*   Live capturing of Youtube Traffic\n",
        "*   Identification of Video Streaming traffic flows\n",
        "*   Rapresentation of downstream and upstream video traffic traces\n",
        "*   Classification of Video Client HTTP Request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAVwWYTrX2rp"
      },
      "outputs": [],
      "source": [
        "# Start the capture and import the pcap/csv file in your notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idGzXjt3X2u1"
      },
      "outputs": [],
      "source": [
        "# Traffic Capture\n",
        "pcap = 'Live_Capture_NetLabMeas.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPmt17znX2xp"
      },
      "outputs": [],
      "source": [
        "# Import pcap in Pandas Dataframe\n",
        "pcap_data = pd.read_csv(pcap, sep=',',encoding='latin-1') #Try to add this if you get encoding problems: encoding='latin-1'\n",
        "pcap_data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OcLOx9zYM6k"
      },
      "source": [
        "##  **Hands On**: Filter Traffic on Video Server's IP Address\n",
        "\n",
        "Find the IP Address(es) of the Video Server(s) contacted by the Video Client.\n",
        "\n",
        "Use DNS information to find the match between Youtube Server's domain name (**googlevideo**) and corresponding IP address."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwlIzMayX20p"
      },
      "outputs": [],
      "source": [
        "# HINT:\n",
        "\n",
        "# 1. E.G., we want to select the string \"polimi\" from a Pandas column vector\n",
        "list_of_strings = pd.Series(['laboratory','polimi', 'network', 'polimi', 'laboratory'])\n",
        "\n",
        "filter = list_of_strings.apply(lambda x: 'polimi' in x)\n",
        "\n",
        "# Access the item at position \"index\" in the vector \"list_of_strings\"\n",
        "print(list_of_strings[filter])\n",
        "\n",
        "# Other examples\n",
        "## Find the index of the desired string in the vector \"list_of_strings\"\n",
        "#index = list_of_strings.str.contains('polimi', regex=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2gICwfiYSby"
      },
      "outputs": [],
      "source": [
        "def filter_traffic(data, domain):\n",
        "\n",
        "    # Look in DNS Responses for googlevideo domain\n",
        "    dns_data = data[data['Protocol']=='DNS']\n",
        "    dns = dns_data[dns_data['Info'].apply(lambda x: 'googlevideo' in x and 'response' in x)]\n",
        "    ips = dns.Address.values\n",
        "    server_names = dns.Name.values\n",
        "\n",
        "    # Filtering on either \"Source\" or \"Destination\" IP, get the\n",
        "    # rows of the dataset that contain at least one of the selected IPs\n",
        "    downlink = data[data['Source'].apply(lambda x: x in ips)].dropna(subset=['Length'])\n",
        "\n",
        "    uplink = data[data['Destination'].apply(lambda x: x in ips)].dropna(subset=['Length'])\n",
        "\n",
        "    return ips, server_names, uplink, downlink\n",
        "\n",
        "## Select IPs of server domain names that include 'googlevideo':\n",
        "domain_name = 'googlevideo'\n",
        "ips, server_names, uplink, downlink = filter_traffic(pcap_data,domain_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uPDDI8GYSeo"
      },
      "outputs": [],
      "source": [
        "print(server_names)\n",
        "print(ips)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APVFrs8jYShs"
      },
      "outputs": [],
      "source": [
        "print(downlink.shape)\n",
        "downlink.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikKJL3SlYeb3"
      },
      "outputs": [],
      "source": [
        "print(uplink.shape)\n",
        "uplink.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHMjHa9CYgPE"
      },
      "source": [
        "## **Hands On**: Find Dominant Traffic Flow\n",
        "Select the dominant traffic flow, i.e., the traffic flows which carries the **majority** of the session's DL traffic.\n",
        "\n",
        "How much data volume ( [MB] ) is downloaded by the Video Client?\n",
        "\n",
        "HINT 1: how to label a traffic flow univocally?\n",
        "\n",
        "HINT 2: check the method groupby() ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUOUUQ2IYeeX"
      },
      "outputs": [],
      "source": [
        "def find_dominant(uplink, downlink):\n",
        "\n",
        "  # Expressed in MB\n",
        "\n",
        "  # Order flows by cumulative DL Volume\n",
        "  flows_DL = downlink.groupby(['Source','Destination'])['Length'].sum()/(10**6)\n",
        "  print(flows_DL)\n",
        "\n",
        "  # Get (Source,Destination) IPs of dominant flow in DL direction\n",
        "  dom_id = flows_DL[flows_DL==max(flows_DL)].index[0]\n",
        "\n",
        "  # Filter traffic selecting the dominant flow (for both DL and UL)\n",
        "  dom_dl = downlink[downlink['Source']==dom_id[0]]\n",
        "  dom_ul = uplink[(uplink['Source']==dom_id[1])]\n",
        "\n",
        "  return dom_ul, dom_dl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dom_ul, dom_dl = find_dominant(uplink, downlink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw4eNXcbYwW0"
      },
      "outputs": [],
      "source": [
        "dom_ul.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIDz5SwiYwZD"
      },
      "outputs": [],
      "source": [
        "dom_dl.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBSwy3BdYuMI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "            1 video       1 hour\n",
        "          (3 mins, MB)\t   (MB)\n",
        "\n",
        "4K (HFR)\t     135         2700\n",
        "\n",
        "1080p\t         83.5        1650\n",
        "\n",
        "720p\t         43.5         870\n",
        "\n",
        "480p\t         13.2         264\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLc1MbZ2Y1AV"
      },
      "source": [
        "## Represent Uplink & Downlink Traffic Traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U_ucw72YuPC"
      },
      "outputs": [],
      "source": [
        "# OPTION 1: MATPLOTLIB\n",
        "# V) easy-to-use\n",
        "# X) not interactive\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "\n",
        "ax.scatter(x=dom_dl['Time'],y=dom_dl['Length'],color='blue',s=15)\n",
        "ax.grid(True)\n",
        "\n",
        "largeul = dom_ul[dom_ul['Length']>100]\n",
        "for x in largeul.index:\n",
        "  plt.axvline(largeul.loc[x,'Time'], ymin=0,\n",
        "              ymax=largeul.loc[x,'Length'], color='red', ls='--')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_l5YAQnYegx"
      },
      "outputs": [],
      "source": [
        "# OPTION 2: PLOTLY\n",
        "# X) require some extra effort\n",
        "# V) nicer (and interactive) GUI\n",
        "\n",
        "x=dom_dl['Time']\n",
        "y=dom_dl['Length']\n",
        "\n",
        "x2=dom_ul['Time']\n",
        "y2=dom_ul['Length']\n",
        "\n",
        "# Select UL Packets larger than 100 Bytes\n",
        "largeul_timestamp = dom_ul[dom_ul['Length']>100].Time.values\n",
        "largeul_size = dom_ul[dom_ul['Length']>100].Length.values\n",
        "\n",
        "# Create trace: one marker per each DL Packet\n",
        "trace = go.Scatter(x = x, y = y,  mode = 'lines+markers', line_shape='hv',\n",
        "                   line=dict(color='#4363d8', width=0.6), name='Downlink')\n",
        "\n",
        "# Create trace: one marker per each UL Packet\n",
        "trace2 = go.Scatter(x = x2, y = y2,  mode = 'markers',\n",
        "                    marker=dict(color='#e6194b'), name='Uplink')\n",
        "\n",
        "# Create trace: vertical line for the first large UL Packet\n",
        "trace3 = go.Scatter(x = [largeul_timestamp[0], largeul_timestamp[0]], y = [-3000, 3000],  mode = 'lines', line_shape='hv',\n",
        "                    line=dict(color='#e6194b', width=0.5, dash='dash'), name='HTTP Request')\n",
        "data = [trace, trace2, trace3]\n",
        "\n",
        "layout = go.Layout(height=800, width=1200, title='Dominant Streaming Flow', xaxis=dict(title='Playback [s]'),\n",
        "                       yaxis=dict(title='Packet Size [bytes]'),legend=dict(orientation=\"h\"))\n",
        "# Plot and embed in ipython notebook!\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# Add vertical lines for each large UL Packet\n",
        "for x in largeul_timestamp[1:]:\n",
        "  fig.add_vline(x=x, line_width=0.5, line_dash=\"dash\", line_color=\"red\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nmTrwJKY-K7"
      },
      "source": [
        "# **Classification of Video Client HTTP Requests**\n",
        "\n",
        "*   Dig into Metadata: a close look into encrypted payloads\n",
        "*   Classification of contents requested by the video client (Audio/Video)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_owaWUf0ZCzd"
      },
      "source": [
        "## Repeat same processing for a different capture\n",
        "\n",
        "The processing applied below repeats what we have done together. The function bodies might be sligthly different from what done during lecture, but the output is exactly the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSpRi7iLY-7U"
      },
      "outputs": [],
      "source": [
        "def filter_traffic_v2(dns_data, traffic_data, domain):\n",
        "\n",
        "    print('Getting Video Server IP...')\n",
        "    # Look in DNS Responses for googlevideo domain\n",
        "    dns = dns_data[dns_data['Domain_Name'].apply(lambda x: 'googlevideo' in x)]\n",
        "    ips = dns.IP.values\n",
        "    server_names = dns.Domain_Name.values\n",
        "\n",
        "    print('Filtering Downstream and Upstream Video Traffic...')\n",
        "    # Filtering on either \"Source\" or \"Destination\" IP, get the\n",
        "    # rows of the dataset that contain at least one of the selected IPs\n",
        "    downlink = traffic_data[traffic_data['SrcIP'].apply(lambda x: x in ips)].dropna(subset=['Size'])\n",
        "\n",
        "    uplink = traffic_data[traffic_data['DstIP'].apply(lambda x: x in ips)].dropna(subset=['Size'])\n",
        "\n",
        "    downlink['Size'] = downlink['Size'].astype(int)\n",
        "    uplink['Size'] = uplink['Size'].astype(int)\n",
        "\n",
        "    return uplink, downlink\n",
        "  \n",
        "def find_dominant_v2(uplink, downlink):\n",
        "\n",
        "  flows_DL = downlink.groupby(['SrcIP','SrcPort','DstIP','DstPort'])['Size'].sum()/(10**6)\n",
        "  flows_UL = uplink.groupby(['SrcIP','SrcPort','DstIP','DstPort'])['Size'].sum()/(10**6)\n",
        "\n",
        "  # Get Dominant Flow ID\n",
        "  dominant_DLflow_id = flows_DL[flows_DL==max(flows_DL)].index[0]\n",
        "  dominant_ULflow_id = flows_UL[flows_UL==max(flows_UL)].index[0]\n",
        "\n",
        "  # Filter out all the other flows\n",
        "  downlink_dominant = downlink.set_index(['SrcIP','SrcPort','DstIP','DstPort']).loc[dominant_DLflow_id]\n",
        "  uplink_dominant = uplink.set_index(['SrcIP','SrcPort','DstIP','DstPort']).loc[dominant_ULflow_id]\n",
        "\n",
        "  return uplink_dominant, downlink_dominant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Application Layer Data: DNS\n",
        "dns = 'dns_pcap_yt_s_1_1005.pcap.log'\n",
        "\n",
        "# Network Layer Data: IP\n",
        "pcap = 'min_out_pcap_yt_s_1_1005.pcap.log'\n",
        "\n",
        "print('Loading data...')\n",
        "pcap_data = pd.read_csv(pcap, sep=';')\n",
        "dns_data = pd.read_csv(dns, sep=';',header=None, names = ['IP','Domain_Name'])\n",
        "\n",
        "print('Pre-Processing data...')\n",
        "uplink, downlink = filter_traffic_v2(dns_data, pcap_data, domain='googlevideo')\n",
        "# Normalize timestamps\n",
        "start = min(uplink['Time']) # smallest UL packet timestamp\n",
        "uplink['Time'] = uplink['Time'] - start\n",
        "downlink['Time'] = downlink['Time'] - start\n",
        "\n",
        "print('Selecting dominant flow...')\n",
        "uplink_dominant, downlink_dominant = find_dominant_v2(uplink, downlink)\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBDOQturZGXh"
      },
      "source": [
        "## Dig Into Metadata\n",
        "\n",
        "How does encrypted packets payload look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BF9otwfY_eV"
      },
      "outputs": [],
      "source": [
        "# Application Layer Data: HTTP\n",
        "\n",
        "measurements = 'yt_s_1_1005.log'\n",
        "http_requests = 'requests_yt_s_1_1005.log'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHdvvTaGY_hG"
      },
      "outputs": [],
      "source": [
        "# Import HTTP Log in Pandas Dataframe\n",
        "http_data = pd.read_csv(http_requests, sep=',',\n",
        "                       names=['time','method','protocol','domain','page',\n",
        "                              'itag','urlparams', 'url']).iloc[1:,:]\n",
        "\n",
        "# Time-Alignment of Application and Network Layer Data\n",
        "http_data['time'] = http_data['time'].astype(float)/1000 - start\n",
        "# Drop Duplicates Rows in Dataset\n",
        "http_data.drop_duplicates(subset=['method','protocol','domain','page','itag','urlparams','url'],\n",
        "                         keep='first', inplace=True)\n",
        "\n",
        "# Show Dataframe\n",
        "http_data.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpm4OggKY_jl"
      },
      "outputs": [],
      "source": [
        "http_data.dropna(subset=['itag']).head(20) # Show only requests for video/audio contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3QAGEuMZNuK"
      },
      "source": [
        "## **Hands On**: Label HTTP Request\n",
        "\n",
        "Label Requests as being for Audio or Video contents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7YKloxaZYYW"
      },
      "outputs": [],
      "source": [
        "audio_itag = list(map(str,[139,140,141,171,172,249,250,251,256,258,325,328]))\n",
        "\n",
        "video_itag = list(map(str,[167,168,169,170,218,219,242,243,244,245,246,247,\n",
        "                  248,271,272,278,302,303,308,313,315,330,331,332,\n",
        "                  333,334,335,336,337,133,134,135,136,137,138,160,\n",
        "                  212,213,214,215,216,217,264,266,298,299]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoUewXaJZYbD"
      },
      "outputs": [],
      "source": [
        "# Hint:\n",
        "temp = pd.DataFrame(['0', '1', '3', '244'],columns=['A'], index=range(4))\n",
        "print(temp)\n",
        "print(temp.isin(video_itag))\n",
        "print(temp[temp['A'].isin(video_itag)])\n",
        "print(temp[temp['A'].isin(video_itag)].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygF5HyHzZb4A"
      },
      "outputs": [],
      "source": [
        "def label_itag(audio_itag, video_itag, data):\n",
        "\n",
        "  data['Content_Type'] = np.zeros(len(data)).astype(str)\n",
        "  for i,j in zip([audio_itag, video_itag],['Audio','Video']):\n",
        "\n",
        "      # Get rows of Audio/Video Itags\n",
        "      index = data[data['itag'].isin(i)].index\n",
        "\n",
        "      # Add to Content Type Column the label (Audio/Video) of the itag at the\n",
        "      # selected rows\n",
        "      data.loc[index,'Content_Type'] = j\n",
        "\n",
        "  return(data)\n",
        "\n",
        "http_data.dropna(inplace=True)\n",
        "http_data = label_itag(audio_itag, video_itag, http_data)\n",
        "http_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNbPi0VPZRkD"
      },
      "source": [
        "## Features Extraction\n",
        "\n",
        "Build a dataset that can be used to perform classification.\n",
        "\n",
        "Consider data from PB_Time = 2 [s] to PB_Time = 180 [s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VvcD-JPZMww"
      },
      "outputs": [],
      "source": [
        "def timebased_filter_v2(data, size=None, min_time=None, max_time=None):\n",
        "  '''\n",
        "  :param data: pd dataframe to be filtered. Must contain columns: \"Size\" and \"Time\"\n",
        "  :param size: all packets shorter than size [Bytes] will be discarded (default 0)\n",
        "  :param min_time: all packets with timestamp smaller than min_time [s] will be discarded (default 0)\n",
        "  :param max_time: all packets with timestamp larger than max_time [s] will be discarded (default 1000)\n",
        "  '''\n",
        "\n",
        "  if size is None:\n",
        "    size=0\n",
        "  if min_time is None:\n",
        "    min_time = 0\n",
        "  if max_time is None:\n",
        "    max_time = 1000\n",
        "\n",
        "  filtered_data = data.copy().reset_index()\n",
        "  mask = (filtered_data['Size']>=size) & (filtered_data['Time']>=min_time) & (filtered_data['Time']<= max_time)\n",
        "  filtered_data = filtered_data.loc[mask[mask ==True].index]\n",
        "\n",
        "  return filtered_data\n",
        "\n",
        "def find_next(array, value):\n",
        "    '''\n",
        "    :param array: np.array, array of floats\n",
        "    :param value: float, reference value\n",
        "    :return: position of the closest element of the array greater than \"value\"\n",
        "    '''\n",
        "    delta = np.asarray(array) - value\n",
        "    idx = np.where(delta >= 0, delta, np.inf).argmin()\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq-3V94b6twq"
      },
      "outputs": [],
      "source": [
        "# Match uplink packets with corresopnding HTTP Request issued by the client\n",
        "\n",
        "x2=uplink_dominant['Time']\n",
        "y2=uplink_dominant['Size']\n",
        "\n",
        "size_threshold = 100 # Bytes\n",
        "largeul_timestamp = uplink_dominant[uplink_dominant['Size']>size_threshold].Time\n",
        "largeul_size = uplink_dominant[uplink_dominant['Size']>size_threshold].Size\n",
        "\n",
        "# Create trace: one vertical dashed line per Large UL Packet\n",
        "i = 0\n",
        "data = []\n",
        "for x in largeul_timestamp[:]:\n",
        "  trace = go.Scatter(x = [x, x], y = [-3000, 3000],\n",
        "                      mode = 'lines', line_shape='hv',\n",
        "                      line=dict(color='#e6194b', width=1),\n",
        "                      name='Large UL P. {}'.format(i))\n",
        "  data.append(trace)\n",
        "  i+=1\n",
        "\n",
        "# Create trace: one marker point per each UL packet\n",
        "trace2 = go.Scatter(x = x2, y = y2,  mode = 'markers',\n",
        "                    marker=dict(color='#e6194b'), name='Uplink Packets')\n",
        "data.append(trace2)\n",
        "layout = go.Layout(height=800, width=1200, title='Focus on HTTP Requests',\n",
        "                   xaxis=dict(title='Playback [s]'),\n",
        "                   legend=dict(orientation=\"v\"))\n",
        "\n",
        "# Plot and embed in ipython notebook!\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "for x in http_data.index:\n",
        "  if http_data.loc[x, 'Content_Type'] in ['Audio']:\n",
        "    fig.add_vline(x=http_data.loc[x, 'time'], line_width=1, line_dash=\"dash\", line_color=\"black\")\n",
        "  elif http_data.loc[x, 'Content_Type'] in ['Video']:\n",
        "    fig.add_vline(x=http_data.loc[x, 'time'], line_width=1, line_dash=\"dot\", line_color=\"black\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfLWQ2l3ZMzj"
      },
      "outputs": [],
      "source": [
        "# Filter UL/DL Data\n",
        "playback_start = 2\n",
        "playback_end = 180\n",
        "min_ul_size = 100\n",
        "min_dl_size = 50\n",
        "ul = timebased_filter_v2(uplink_dominant, min_ul_size, playback_start, playback_end)\n",
        "dl = timebased_filter_v2(downlink_dominant, min_dl_size, playback_start, playback_end)\n",
        "# ****************************************************************************\n",
        "# Create an empty dataset\n",
        "\n",
        "dataset = pd.DataFrame(columns=['Request_Size','Inter_RR_Time','DL_Time','DL_Vol','DL_Size','PB_Time'])\n",
        "\n",
        "# ****************************************************************************\n",
        "# Feature 1: Client Request Size\n",
        "\n",
        "dataset['Request_Size'] = list(ul.Size.values)\n",
        "\n",
        "# ****************************************************************************\n",
        "# Feature 2: Inter Request-Response Time\n",
        "\n",
        "rr_time = []\n",
        "response_time = []\n",
        "for t in ul.Time:\n",
        "  response_time.append(find_next(dl.Time, t)) #index of next DL packet timestamp\n",
        "  rr_time.append(dl.Time.iloc[response_time[-1]] - t)\n",
        "\n",
        "dataset['Inter_RR_Time'] = rr_time\n",
        "\n",
        "# ****************************************************************************\n",
        "# Feature 3-4-5: Download Time, Download Volume, Download Size (# Packets)\n",
        "\n",
        "dt = []\n",
        "dv = []\n",
        "ds = []\n",
        "\n",
        "for rt1, t, rt2 in zip(response_time[:-1], ul.Time.iloc[1:], response_time[1:]):\n",
        "\n",
        "  #Download Time\n",
        "  dt.append(dl.Time.iloc[rt2-1] - dl.Time.iloc[rt1])\n",
        "  #print(dl.Time.iloc[rt1], dl.Time.iloc[rt2-1], t, dl.Time.iloc[rt2],)\n",
        "\n",
        "  temp = timebased_filter_v2(dl, 0, dl.Time.iloc[rt1], dl.Time.iloc[rt2-1])\n",
        "  #Download Volume\n",
        "  dv.append(temp.Size.sum())\n",
        "\n",
        "  #Download Size (# Packets)\n",
        "  ds.append(temp.shape[0])\n",
        "\n",
        "\n",
        "# Consider also last HTTP iteration\n",
        "#Download Time\n",
        "dt.append(dl.Time.iloc[-1] - dl.Time.iloc[rt2])\n",
        "\n",
        "temp = timebased_filter_v2(dl, 0, dl.Time.iloc[rt2], dl.Time.iloc[-1])\n",
        "#Download Volume\n",
        "dv.append(temp.Size.sum())\n",
        "\n",
        "#Download Size (# Packets)\n",
        "ds.append(temp.shape[0])\n",
        "\n",
        "\n",
        "dataset['DL_Time'] = dt\n",
        "dataset['DL_Vol'] = dv\n",
        "dataset['DL_Size'] = ds\n",
        "\n",
        "# ****************************************************************************\n",
        "# Feature 5: Playback Time\n",
        "\n",
        "pbt = list(ul.Time.values)\n",
        "dataset['PB_Time'] = pbt\n",
        "\n",
        "# ****************************************************************************\n",
        "# Check Features Consistency\n",
        "dataset = dataset[(dataset > 0).all(1)]\n",
        "dataset = dataset[dataset['DL_Time']<20]\n",
        "# ****************************************************************************\n",
        "\n",
        "print(dataset.shape)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7WqGYEzZ6lM"
      },
      "outputs": [],
      "source": [
        "# Filter HTTP Data to get groundtruth\n",
        "mask = (http_data['time']>=playback_start) & (http_data['time']<= playback_end)\n",
        "\n",
        "groundtruth = http_data.loc[mask[mask == True].index].loc[:,['Content_Type']].reset_index(drop=True)\n",
        "\n",
        "print(groundtruth.shape)\n",
        "groundtruth.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RguBADPTaAd4"
      },
      "source": [
        "## Classification of HTTP Requests\n",
        "\n",
        "Compare the performance of two classification scenarios:\n",
        "\n",
        "\n",
        "1.   Use as groundtruth a **random** vector of zeros and ones\n",
        "2.   Use as groundtruth the **true** vector of HTTP requests labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8fRYaPbaC20"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "\n",
        "def normalize_dataset(training_set, test_set):\n",
        "\n",
        "  mean_train = training_set.mean()\n",
        "  std_train = training_set.std()\n",
        "  norm_train = (training_set - mean_train)/std_train\n",
        "  norm_test = (test_set - mean_train)/std_train\n",
        "\n",
        "  return norm_train, norm_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW95qJ51aC-y"
      },
      "outputs": [],
      "source": [
        "# generate a random vector of zeros and ones\n",
        "s = pd.Series(np.random.randint(2, size=(25,)))\n",
        "s[s==0] = 'Audio'\n",
        "s[s==1] = 'Video'\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLA9n7uhaA3B"
      },
      "outputs": [],
      "source": [
        "accuracy_knn = []\n",
        "accuracy_rf = []\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "raw_knn = KNeighborsClassifier(1) # KNN Classifier\n",
        "raw_rf = RandomForestClassifier() # Random Forest Classifier\n",
        "\n",
        "for gt,case in zip([s, groundtruth],['Random', 'Video Streaming']):\n",
        "\n",
        "  for train_index, test_index in kf.split(dataset):\n",
        "\n",
        "    # Get Training and Test Set\n",
        "    data_train, data_test = dataset.iloc[train_index,:], dataset.iloc[test_index,:]\n",
        "    labels_train, labels_test = gt.iloc[train_index], gt.iloc[test_index]\n",
        "\n",
        "    # Normalize datasets: it benefits the learning process\n",
        "    norm_train, norm_test = normalize_dataset(data_train, data_test)\n",
        "\n",
        "    # Get Classifiers\n",
        "    raw_knn = KNeighborsClassifier(1)\n",
        "    raw_rf = RandomForestClassifier()\n",
        "    #raw_lr = LogisticRegression()\n",
        "\n",
        "    # Train the Classifiers\n",
        "    kn = raw_knn.fit(norm_train, labels_train) # fit knn classifier on training set\n",
        "    rf = raw_rf.fit(norm_train, labels_train) # fit clf classifier on training set\n",
        "\n",
        "    # Predict\n",
        "    prediction_kn = kn.predict(norm_test)\n",
        "    prediction_rf = rf.predict(norm_test)\n",
        "\n",
        "    # Collect Results\n",
        "    accuracy_knn.append(metrics.accuracy_score(labels_test, prediction_kn))\n",
        "    accuracy_rf.append(metrics.accuracy_score(labels_test, prediction_rf))\n",
        "\n",
        "  print(\"############\\n\")\n",
        "  print(\"Case: {}\\n\".format(case))\n",
        "  print(\"Detection Performance (KNN):\\n\")\n",
        "  print(\"Accuracy = : {} (std = {})\\n\".format(np.mean(accuracy_knn), np.std(accuracy_knn)))\n",
        "  print(\"Detection Performance (Random Forest):\\n\")\n",
        "  print(\"Accuracy = : {} (std = {})\\n\".format(np.mean(accuracy_rf), np.std(accuracy_rf)))\n",
        "  print(\"############\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoj8o7aQabUT"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import joblib\n",
        "\n",
        "groundtruth_v2 = groundtruth.copy()\n",
        "groundtruth_v2[groundtruth == 'Audio'] = 0\n",
        "groundtruth_v2[groundtruth == 'Video'] = 1\n",
        "\n",
        "# Use a Classifier already Trained to make prediction\n",
        "custom_rf = joblib.load(\"finalized_model.joblib\")\n",
        "\n",
        "# Features Mean and Std. values found in the Training Set\n",
        "mean = [609.442804, 0.448292, 0.638842, 505372.314610, 377.254203, 56.366221]\n",
        "std = [77.769301, 1.698097, 1.128774, 595930.279100, 445.374095, 42.143888]\n",
        "\n",
        "# Normalize test set\n",
        "test = (dataset - mean)/std\n",
        "\n",
        "# Predict\n",
        "prediction = custom_rf.predict(test)\n",
        "\n",
        "# Post Processing\n",
        "prediction = [int(x) for x in prediction]\n",
        "groundtruth_v2 = [int(x) for x in groundtruth_v2.values]\n",
        "\n",
        "print(\"############\\n\")\n",
        "print(\"Case: {}\\n\".format('Custom Classifier'))\n",
        "print(\"Detection Performance:\\n\")\n",
        "print(\"Accuracy = : {}\\n\".format(metrics.accuracy_score(groundtruth_v2, prediction)))\n",
        "print(\"############\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8OcLOx9zYM6k",
        "AHMjHa9CYgPE",
        "RLc1MbZ2Y1AV",
        "_owaWUf0ZCzd",
        "I3QAGEuMZNuK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
